{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3f8b5c16e7eb563",
   "metadata": {},
   "source": [
    "# Ejercicio 6: Dense Retrieval e Introducción a FAISS\n",
    "\n",
    "## Objetivo de la práctica\n",
    "\n",
    "Generar embeddings con sentence-transformers (SBERT, E5), e indexar documentos con FAISS "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdd69ed7fcbeef9d",
   "metadata": {},
   "source": [
    "## Parte 0: Carga del Corpus\n",
    "### Actividad\n",
    "\n",
    "1. Carga el corpus 20 Newsgroups desde sklearn.datasets.fetch_20newsgroups.\n",
    "2. Limita el corpus a los primeros 2000 documentos para facilitar el procesamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b00fbde6cfc88b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando corpus 20 Newsgroups...\n",
      "Total de documentos cargados: 2000\n",
      "Ejemplo de documento:\n",
      "\n",
      "\n",
      "I am sure some bashers of Pens fans are pretty confused about the lack\n",
      "of any kind of posts about the recent Pens massacre of the Devils. Actually,\n",
      "I am  bit puzzled too and a bit relieved. However,...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Cargar el corpus 20 Newsgroups\n",
    "print(\"Cargando corpus 20 Newsgroups...\")\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Limitar a los primeros 2000 documentos\n",
    "documents = newsgroups.data[:2000]\n",
    "print(f\"Total de documentos cargados: {len(documents)}\")\n",
    "print(f\"Ejemplo de documento:\\n{documents[0][:200]}...\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9184f4b3e66e20a",
   "metadata": {},
   "source": [
    "## Parte 2: Generación de Embeddings\n",
    "### Actividad\n",
    "\n",
    "1. Usa dos modelos de sentence-transformers. Puedes usar: `'all-MiniLM-L6-v2'` (SBERT), o `'intfloat/e5-base'` (E5). Cuando uses E5, antepon `\"passage: \"` a cada documento antes de codificar.\n",
    "2. Genera los vectores de embeddings para todos los documentos usando el modelo seleccionado.\n",
    "3. Guarda los embeddings en un array de NumPy para su posterior indexación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ae4a00fa-4aa3-4eb4-9b78-ca41797b1802",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (5.1.2)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.57.3)\n",
      "Requirement already satisfied: tqdm in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.9.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.15.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.36.0)\n",
      "Requirement already satisfied: Pillow in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.1.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: requests in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.2)\n",
      "Requirement already satisfied: sympy>=1.13.3 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
      "Requirement already satisfied: networkx>=2.5.1 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: setuptools in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (72.1.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from requests->transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.11.12)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\labp4e010\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "525ae7515c6169d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando modelo SBERT: all-MiniLM-L6-v2...\n",
      "Generando embeddings con SBERT...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f246e912dad41a19f1bfa16338633c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/63 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensión de embeddings SBERT: (2000, 384)\n"
     ]
    }
   ],
   "source": [
    "# Opción 1: Usando SBERT (all-MiniLM-L6-v2)\n",
    "from sentence_transformers import SentenceTransformer\n",
    "print(\"Cargando modelo SBERT: all-MiniLM-L6-v2...\")\n",
    "model_sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "print(\"Generando embeddings con SBERT...\")\n",
    "doc_embeddings_sbert = model_sbert.encode(documents, show_progress_bar=True)\n",
    "print(f\"Dimensión de embeddings SBERT: {doc_embeddings_sbert.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0636b273-0616-4ff3-887c-fd1c9706a89b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Embeddings guardados exitosamente\n"
     ]
    }
   ],
   "source": [
    "# Guardar embeddings en arrays de NumPy\n",
    "np.save('embeddings_sbert.npy', doc_embeddings_sbert)\n",
    "print(\"\\nEmbeddings guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8810c674-0faa-4141-814c-3e23aef61359",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Opción 2: Usando E5 (intfloat/e5-base)\n",
    "print(\"\\nCargando modelo E5: intfloat/e5-base...\")\n",
    "model_e5 = SentenceTransformer('intfloat/e5-base')\n",
    " \n",
    "# Preprocesar documentos con prefijo \"passage: \" para E5\n",
    "documents_e5 = [\"passage: \" + doc for doc in documents]\n",
    "\n",
    "print(\"Generando embeddings con E5...\")\n",
    "doc_embeddings_e5 = model_e5.encode(documents_e5, show_progress_bar=True)\n",
    "print(f\"Dimensión de embeddings E5: {doc_embeddings_e5.shape}\")\n",
    "\n",
    "# Guardar embeddings en arrays de NumPy\n",
    "np.save('embeddings_e5.npy', doc_embeddings_e5)\n",
    "print(\"\\nEmbeddings guardados exitosamente\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e53b50365064d2b1",
   "metadata": {},
   "source": [
    "## Parte 3: Indexación con FAISS\n",
    "### Actividad\n",
    "\n",
    "1. Crea un índice plano con faiss.IndexFlatL2 para búsquedas por distancia euclidiana.\n",
    "2. Asegúrate de usar la dimensión correcta `(embedding_dim = doc_embeddings.shape[1])`.\n",
    "3. Agrega los vectores de documentos al índice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c723e6189ab1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "40462a067ca2d379",
   "metadata": {},
   "source": [
    "## Parte 4: Consulta Semántica\n",
    "### Actividad\n",
    "\n",
    "1. Escribe una consulta en lenguaje natural. Ejemplos:\n",
    "\n",
    "    * \"God, religion, and spirituality\"\n",
    "    * \"space exploration\"\n",
    "    * \"car maintenance\"\n",
    "\n",
    "2. Codifica la consulta utilizando el mismo modelo de embeddings. Cuando uses E5, antepon `\"query: \"` a la consulta.\n",
    "3. Recupera los 5 documentos más relevantes con `index.search(...)`.\n",
    "4. Muestra los textos de los documentos recuperados (puedes mostrar solo los primeros 500 caracteres de cada uno)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aad085806124c709",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_documents(query, model, doc_embeddings, documents, top_k=5, use_e5=False):\n",
    "    # Codificar la consulta\n",
    "    if use_e5:\n",
    "        query_text = \"query: \" + query\n",
    "    else:\n",
    "        query_text = query\n",
    "    \n",
    "    query_embedding = model.encode([query_text])\n",
    "    \n",
    "    # Calcular similitud coseno\n",
    "    similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
    "    \n",
    "    # Obtener los índices de los top_k documentos más similares\n",
    "    top_indices = np.argsort(similarities)[::-1][:top_k]\n",
    "    \n",
    "    return top_indices, similarities[top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be897fb2-b841-40c2-ad79-ad19c2351f7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "BÚSQUEDAS CON SBERT (all-MiniLM-L6-v2)\n",
      "======================================================================\n",
      "CONSULTA: 'God, religion, and spirituality'\n",
      "Resultado #1 (Similitud: 0.4150)\n",
      "Documento #996\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Humanist, or sub-humanist? :-)...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #2 (Similitud: 0.3307)\n",
      "Documento #282\n",
      "\n",
      "I didn't know God was a secular humanist...\n",
      "\n",
      "Kent...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #3 (Similitud: 0.3013)\n",
      "Documento #677\n",
      " \n",
      "(Deletion)\n",
      " \n",
      "For me, it is a \"I believe no gods exist\" and a \"I don't believe gods exist\".\n",
      " \n",
      "In other words, I think that statements like gods are or somehow interfere\n",
      "with this world are false or meaningless. In Ontology, one can fairly\n",
      "conclude that when \"A exist\" is meaningless A does not exist. Under the\n",
      "Pragmatic definition of truth, \"A exists\" is meaningless makes A exist\n",
      "even logically false.\n",
      " \n",
      "A problem with such statements is that one can't disprove a subjective god\n",
      "by definition, and...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #4 (Similitud: 0.2878)\n",
      "Documento #943\n",
      "\n",
      "\n",
      "Atoms are not objective.  They aren't even real.  What scientists call\n",
      "an atom is nothing more than a mathematical model that describes \n",
      "certain physical, observable properties of our surroundings.  All\n",
      "of which is subjective.  \n",
      "\n",
      "What is objective, though, is the approach a scientist \n",
      "takes in discussing his model and his observations.  There\n",
      "is no objective science.  But there is an objective approach\n",
      "which is subjectively selected by the scientist.  Objective\n",
      "in this case means a specified, ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #5 (Similitud: 0.2856)\n",
      "Documento #791\n",
      "Above all, love each other deeply, because love covers over a multitude of\n",
      "sins. ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CONSULTA: 'space exploration'\n",
      "Resultado #1 (Similitud: 0.4991)\n",
      "Documento #495\n",
      "I am posting this for a friend without internet access. Please inquire\n",
      "to the phone number and address listed.\n",
      "---------------------------------------------------------------------\n",
      "\n",
      "\"Space: Teaching's Newest Frontier\"\n",
      "Sponsored by the Planetary Studies Foundation\n",
      "\n",
      "The Planetary Studies Foundation is sponsoring a one week class for\n",
      "teachers called \"Space: Teaching's Newest Frontier.\" The class will be\n",
      "held at the Sheraton Suites in Elk Grove, Illinois from June 14 through\n",
      "June 18. Participants wh...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #2 (Similitud: 0.4398)\n",
      "Documento #1643\n",
      "\n",
      "Well, here goes.\n",
      "\n",
      "The first item of business is to establish the importance space life\n",
      "sciences in the whole of scheme of humankind.  I mean compared\n",
      "to football and baseball, the average joe schmoe doesn't seem interested\n",
      "or even curious about spaceflight.  I think that this forum can\n",
      "make a major change in that lack of insight and education.\n",
      "\n",
      "All of us, in our own way, can contribute to a comprehensive document\n",
      "which can be released to the general public around the world.  The\n",
      "document would ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #3 (Similitud: 0.4321)\n",
      "Documento #786\n",
      "Ron Miller is a space artist with a long and distinguished career.  \n",
      "I've admired both his paintings (remember the USPS Solar System\n",
      "Exploration Stamps last year?) and his writings on the history of\n",
      "spaceflight.  For several years he's been working on a *big* project\n",
      "which is almost ready to hit the streets.  A brochure from his\n",
      "publisher has landed in my mailbox, and I thought it was cool enough\n",
      "to type in part of it (it's rather long).  Especially given the Net's\n",
      "strong interest in vaporware s...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #4 (Similitud: 0.3995)\n",
      "Documento #1199\n",
      "Any comments on the absorbtion of the Office of Exploration into the\n",
      "Office of Space Sciences and the reassignment of Griffin to the \"Chief\n",
      "Engineer\" position?  Is this just a meaningless administrative\n",
      "shuffle, or does this bode ill for SEI?\n",
      "\n",
      "In my opinion, this seems like a Bad Thing, at least on the surface.\n",
      "Griffin seemed to be someone who was actually interested in getting\n",
      "things done, and who was willing to look an innovative approaches to\n",
      "getting things done faster, better, and cheaper.  ...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #5 (Similitud: 0.3746)\n",
      "Documento #25\n",
      "AW&ST  had a brief blurb on a Manned Lunar Exploration confernce\n",
      "May 7th  at Crystal City Virginia, under the auspices of AIAA.\n",
      "\n",
      "Does anyone know more about this?  How much, to attend????\n",
      "\n",
      "Anyone want to go?...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "CONSULTA: 'car maintenance'\n",
      "Resultado #1 (Similitud: 0.4879)\n",
      "Documento #1822\n",
      "As you can see, I have two 1987 cars, both worth about $3000 each.\n",
      "The problem is that maintenance costs on these two cars is\n",
      "running about $4000 per year and insurance $3000 per year.\n",
      "\n",
      "What am I doing wrong?\n",
      "\n",
      "Within the last two months, the follows costs have occured:\n",
      "\n",
      "Dodge 600 SE (Dodge's attempt at the American German car!)\n",
      "\n",
      "$1,000 - replace head gasket\n",
      "$300   - new radiator\n",
      "\n",
      "Chevy Nova CL (Chevy's attempt at a Japan import!)\n",
      "\n",
      "$500 - tune-up,oil change,valve gasket,middle exhaust pipe, misc....\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #2 (Similitud: 0.4467)\n",
      "Documento #1470\n",
      "Archive-name: rec-autos/part1\n",
      "\n",
      "[most recent changes, 15 March 1993: addition of alt.autos.karting -- rpw]\n",
      "\n",
      "               === Welcome to Rec.Autos.* ===\n",
      "\n",
      "This article is sent out automatically each month, and contains a general\n",
      "description of the purpose of each of the automotive newsgroups, and\n",
      "some suggested guidelines for discussions.  The keywords `Monthly Posting'\n",
      "will always appear to make killing this article easy for users of\n",
      "newsreaders with kill facilities.  This article is posted to a...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #3 (Similitud: 0.4061)\n",
      "Documento #43\n",
      "Archive-name: rec-autos/part3\n",
      "\n",
      "The Automotive Articles Archive Server:\n",
      "\n",
      "the automotive archive server is in the process of being rehosted,\n",
      "and is presently not available....\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #4 (Similitud: 0.3863)\n",
      "Documento #1368\n",
      "} maintenance) and probably didn't know the answer at the start of the thread.\n",
      "\n",
      "\tUh, Doug, I don't know what school of thought your from, but chain \n",
      "drive are MUCH more efficient than shafties.  End of story.  Period.\n",
      "\tBut I will give you that shafties are much less maintenance intensive...\n",
      "\n",
      "\n",
      "\t\t\t\t\t\tEthan...\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "Resultado #5 (Similitud: 0.3806)\n",
      "Documento #1296\n",
      "\n",
      "There was a Volvo owner that had $3000 dollars worth of improvements to the \n",
      "looks of the car by hail :).\n",
      "...\n",
      "----------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "queries = [\n",
    "    \"God, religion, and spirituality\",\n",
    "    \"space exploration\",\n",
    "    \"car maintenance\"\n",
    "]\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BÚSQUEDAS CON SBERT (all-MiniLM-L6-v2)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"CONSULTA: '{query}'\")\n",
    "    \n",
    "    indices, scores = search_documents(\n",
    "        query, \n",
    "        model_sbert, \n",
    "        doc_embeddings_sbert, \n",
    "        documents, \n",
    "        top_k=5\n",
    "    )\n",
    "    \n",
    "    for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "        print(f\"Resultado #{rank} (Similitud: {score:.4f})\")\n",
    "        print(f\"Documento #{idx}\")\n",
    "        print(f\"{documents[idx][:500]}...\")\n",
    "        print(f\"{'-'*70}\\n\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3c6d47-7f4e-47cb-b1a6-f1c6d0a658c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"BÚSQUEDAS CON E5 (intfloat/e5-base)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\n{'='*70}\")\n",
    "    print(f\"CONSULTA: '{query}'\")\n",
    "    print(f\"{'='*70}\\n\")\n",
    "    \n",
    "    indices, scores = search_documents(\n",
    "        query, \n",
    "        model_e5, \n",
    "        doc_embeddings_e5, \n",
    "        documents, \n",
    "        top_k=5,\n",
    "        use_e5=True\n",
    "    )\n",
    "    \n",
    "    for rank, (idx, score) in enumerate(zip(indices, scores), 1):\n",
    "        print(f\"Resultado #{rank} (Similitud: {score:.4f})\")\n",
    "        print(f\"Documento #{idx}\")\n",
    "        print(f\"{documents[idx][:500]}...\")\n",
    "        print(f\"{'-'*70}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e2d1879-3f25-47b4-a7d9-375ab9d45ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================\n",
    "# Comparación de resultados\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ANÁLISIS COMPARATIVO\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "test_query = \"space exploration\"\n",
    "print(f\"\\nComparando resultados para: '{test_query}'\\n\")\n",
    "\n",
    "# SBERT\n",
    "indices_sbert, scores_sbert = search_documents(\n",
    "    test_query, model_sbert, doc_embeddings_sbert, documents, top_k=5\n",
    ")\n",
    "\n",
    "# E5\n",
    "indices_e5, scores_e5 = search_documents(\n",
    "    test_query, model_e5, doc_embeddings_e5, documents, top_k=5, use_e5=True\n",
    ")\n",
    "\n",
    "print(\"Top 5 con SBERT:\")\n",
    "for rank, (idx, score) in enumerate(zip(indices_sbert, scores_sbert), 1):\n",
    "    print(f\"  {rank}. Doc #{idx} - Similitud: {score:.4f}\")\n",
    "\n",
    "print(\"\\nTop 5 con E5:\")\n",
    "for rank, (idx, score) in enumerate(zip(indices_e5, scores_e5), 1):\n",
    "    print(f\"  {rank}. Doc #{idx} - Similitud: {score:.4f}\")\n",
    "\n",
    "# Calcular overlap\n",
    "overlap = len(set(indices_sbert) & set(indices_e5))\n",
    "print(f\"\\nDocumentos en común: {overlap}/5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc9e5e7815c7508",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
