{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "header",
      "metadata": {
        "id": "header"
      },
      "source": [
        "# Examen - Sistema de Recuperación de Información\n",
        "\n",
        "**Nombre:** Josune Singaña\n",
        "**Fecha:** 28 enero 2025\n",
        "\n",
        "Notebook implementa un sistema completo de Recuperación de Información utilizando:\n",
        "- **Dataset:** rXiv Dataset. Scholarly articles, from the vast branches of physics to the many subdisciplines of computer science\n",
        "- **Pipeline:** Preprocesamiento -Embeddings-Búsqueda Vectorial-Re-ranking\n",
        "- **Evaluación:** Precision@k, Recall@k usando qrels creadas\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "install",
      "metadata": {
        "id": "install"
      },
      "source": [
        "## 1. Instalación de Dependencias\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "install_libs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "install_libs",
        "outputId": "36c39ba8-8581-4462-dd24-d10365697a94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ir-datasets in /usr/local/lib/python3.12/dist-packages (0.5.11)\n",
            "Requirement already satisfied: beautifulsoup4>=4.4.1 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (4.13.5)\n",
            "Requirement already satisfied: inscriptis>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (2.7.0)\n",
            "Requirement already satisfied: lxml>=4.5.2 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (5.4.0)\n",
            "Requirement already satisfied: numpy>=1.18.1 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (2.0.2)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (6.0.3)\n",
            "Requirement already satisfied: requests>=2.22.0 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (4.67.1)\n",
            "Requirement already satisfied: trec-car-tools>=2.5.4 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (2.6)\n",
            "Requirement already satisfied: lz4>=3.1.10 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (4.4.5)\n",
            "Requirement already satisfied: warc3-wet>=0.2.3 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (0.2.5)\n",
            "Requirement already satisfied: warc3-wet-clueweb09>=0.2.5 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (0.2.5)\n",
            "Requirement already satisfied: zlib-state>=0.1.3 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (0.1.10)\n",
            "Requirement already satisfied: ijson>=3.1.3 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (3.4.0.post0)\n",
            "Requirement already satisfied: unlzw3>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (0.2.3)\n",
            "Requirement already satisfied: pyarrow>=16.1.0 in /usr/local/lib/python3.12/dist-packages (from ir-datasets) (18.1.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (2.8.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.4.1->ir-datasets) (4.15.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir-datasets) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir-datasets) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir-datasets) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.22.0->ir-datasets) (2026.1.4)\n",
            "Requirement already satisfied: cbor>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from trec-car-tools>=2.5.4->ir-datasets) (1.0.0)\n",
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.2.0)\n",
            "Requirement already satisfied: transformers<6.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.4)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (2025.11.3)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<6.0.0,>=4.41.0->sentence-transformers) (0.7.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2026.1.4)\n",
            "Requirement already satisfied: faiss-cpu in /usr/local/lib/python3.12/dist-packages (1.13.2)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.3.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.3)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2025.11.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.12/dist-packages (1.7.4.5)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.12/dist-packages (from kaggle) (6.3.0)\n",
            "Requirement already satisfied: certifi>=14.05.14 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2026.1.4)\n",
            "Requirement already satisfied: charset-normalizer in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.4.4)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from kaggle) (3.11)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.12/dist-packages (from kaggle) (5.29.5)\n",
            "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.9.0.post0)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.12/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.32.4)\n",
            "Requirement already satisfied: setuptools>=21.0.0 in /usr/local/lib/python3.12/dist-packages (from kaggle) (75.2.0)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.17.0)\n",
            "Requirement already satisfied: text-unidecode in /usr/local/lib/python3.12/dist-packages (from kaggle) (1.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from kaggle) (4.67.1)\n",
            "Requirement already satisfied: urllib3>=1.15.1 in /usr/local/lib/python3.12/dist-packages (from kaggle) (2.5.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.12/dist-packages (from kaggle) (0.5.1)\n",
            "✓ Instalación completada\n"
          ]
        }
      ],
      "source": [
        "# Instalación de librerías necesarias\n",
        "!pip install ir-datasets\n",
        "!pip install sentence-transformers\n",
        "!pip install faiss-cpu\n",
        "!pip install nltk\n",
        "!pip install pandas\n",
        "!pip install numpy\n",
        "!pip install tqdm\n",
        "!pip install scikit-learn\n",
        "!pip install kaggle\n",
        "\n",
        "print(\"✓ Instalación completada\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "imports",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imports",
        "outputId": "bdd434c4-e6a8-4f89-fa68-86681d414c1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Librerías importadas exitosamente\n"
          ]
        }
      ],
      "source": [
        "# Importar todas las librerías\n",
        "import ir_datasets\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import re\n",
        "import faiss\n",
        "from tqdm import tqdm\n",
        "from collections import defaultdict\n",
        "from sentence_transformers import SentenceTransformer, CrossEncoder\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Descargar recursos de NLTK\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "print(\"✓ Librerías importadas exitosamente\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec1_md",
      "metadata": {
        "id": "sec1_md"
      },
      "source": [
        "## 2. Descarga y carga del dataset arXiv\n",
        "\n",
        "En esta sección se descarga y carga el dataset **arXiv Metadata**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# Configurar credenciales (Sube tu kaggle.json a Colab )\n",
        "os.environ['KAGGLE_CONFIG_DIR'] = \"/content\" # O la ruta donde esté el json\n",
        "# Descargar usando la API\n",
        "!kaggle datasets download -d Cornell-University/arxiv\n",
        "# Descomprimir\n",
        "!unzip arxiv.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zBYDG1HGhubM",
        "outputId": "f477098d-73e4-4608-e883-1c30cd4f1885"
      },
      "id": "zBYDG1HGhubM",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Warning: Your Kaggle API key is readable by other users on this system! To fix this, you can run 'chmod 600 /content/kaggle.json'\n",
            "Dataset URL: https://www.kaggle.com/datasets/Cornell-University/arxiv\n",
            "License(s): CC0-1.0\n",
            "Downloading arxiv.zip to /content\n",
            " 97% 1.51G/1.56G [00:10<00:01, 38.4MB/s]\n",
            "100% 1.56G/1.56G [00:11<00:00, 152MB/s] \n",
            "Archive:  arxiv.zip\n",
            "  inflating: arxiv-metadata-oai-snapshot.json  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "load_dataset",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "load_dataset",
        "outputId": "5170881d-f3c2-4e46-edbe-c180aa82f999"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset cargado: arXiv\n",
            "Total de documentos cargados: 50000\n"
          ]
        }
      ],
      "source": [
        "# Cargar el dataset arXiv el archivo arxiv-metadata-oai-snapshot.json a Colab\n",
        "import json\n",
        "\n",
        "# Cargar el archivo JSON\n",
        "arxiv_data = []\n",
        "\n",
        "# Lee el archivo línea\n",
        "with open('arxiv-metadata-oai-snapshot.json', 'r') as f:\n",
        "    for i, line in enumerate(f):\n",
        "        if i >= 50000:  # Limitar a 50k documentos\n",
        "            break\n",
        "        arxiv_data.append(json.loads(line))\n",
        "\n",
        "print(f\"Dataset cargado: arXiv\")\n",
        "print(f\"Total de documentos cargados: {len(arxiv_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "explore_queries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "explore_queries",
        "outputId": "a61f5e25-2d51-4769-a1ae-6870a12bb8ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "EJEMPLO DE DOCUMENTOS\n",
            "Document ID: 0704.0001\n",
            "Título: Calculation of prompt diphoton production cross sections at Tevatron and\n",
            "  LHC energies\n",
            "Categorías: hep-ph\n",
            "Abstract (primeros 300 caracteres):   A fully differential calculation in perturbative quantum chromodynamics is presented for the production of massive photon pairs at hadron colliders. All next-to-leading order perturbative contributions from quark-antiquark, gluon-(anti)quark, and gluon-gluon subprocesses are included, as well as a...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Document ID: 0704.0002\n",
            "Título: Sparsity-certifying Graph Decompositions\n",
            "Categorías: math.CO cs.CG\n",
            "Abstract (primeros 300 caracteres):   We describe a new algorithm, the $(k,\\ell)$-pebble game with colors, and use it obtain a characterization of the family of $(k,\\ell)$-sparse graphs and algorithmic solutions to a family of problems concerning tree decompositions of graphs. Special instances of sparse graphs appear in rigidity theo...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Document ID: 0704.0003\n",
            "Título: The evolution of the Earth-Moon system based on the dark matter field\n",
            "  fluid model\n",
            "Categorías: physics.gen-ph\n",
            "Abstract (primeros 300 caracteres):   The evolution of Earth-Moon system is described by the dark matter field fluid model proposed in the Meeting of Division of Particle and Field 2004, American Physical Society. The current behavior of the Earth-Moon system agrees with this model very well and the general pattern of the evolution of...\n",
            "--------------------------------------------------------------------------------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Ejemplo documentos\n",
        "print(\"EJEMPLO DE DOCUMENTOS\")\n",
        "\n",
        "for i in range(min(3, len(arxiv_data))):\n",
        "    doc = arxiv_data[i]\n",
        "    print(f\"Document ID: {doc['id']}\")\n",
        "    print(f\"Título: {doc['title']}\")\n",
        "    print(f\"Categorías: {doc['categories']}\")\n",
        "    abstract = doc['abstract'].replace('\\n', ' ')\n",
        "    print(f\"Abstract (primeros 300 caracteres): {abstract[:300]}...\")\n",
        "    print(\"-\" * 80 + \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec2_md",
      "metadata": {
        "id": "sec2_md"
      },
      "source": [
        "## 3. Preprocesamiento de Datos\n",
        "\n",
        "Aplicamos las siguientes técnicas de preprocesamiento:\n",
        "1. **Tokenización:** División del texto en palabras\n",
        "2. **Normalización:** Conversión a minúsculas\n",
        "3. **Eliminación de stopwords:** Remover palabras comunes sin significado\n",
        "4. **Stemming:** Reducir palabras a su raíz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "preprocess_functions",
      "metadata": {
        "id": "preprocess_functions"
      },
      "outputs": [],
      "source": [
        "# Inicializar herramientas de preprocesamiento\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def clean_text(text):\n",
        "    # Convertir a minúsculas\n",
        "    text = text.lower()\n",
        "    # Eliminar caracteres especiales y números, conservar solo letras y espacios\n",
        "    text = re.sub(r'[^a-z\\s]', ' ', text)\n",
        "    # Eliminar espacios múltiples\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "def tokenize(text):\n",
        "# Tokeniza el texto en palabras individuales.\n",
        "    return text.split()\n",
        "\n",
        "def remove_stopwords(tokens):\n",
        "# Elimina las stopwords de la lista de tokens.\n",
        "    return [word for word in tokens if word not in stop_words and len(word) > 2]\n",
        "\n",
        "def apply_stemming(tokens):\n",
        "# Aplica stemming a cada token.\n",
        "    return [stemmer.stem(token) for token in tokens]\n",
        "\n",
        "def preprocess_text(text):\n",
        "# Pipeline completo de preprocesamiento.\n",
        "    text = clean_text(text)\n",
        "    tokens = tokenize(text)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    tokens = apply_stemming(tokens)\n",
        "    return ' '.join(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "test_preprocess",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_preprocess",
        "outputId": "7703978f-36bf-41d7-aca7-ec4b107d894f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Texto original:\n",
            "The International Conflict in the Middle East has escalated dramatically!\n",
            "\n",
            "Texto procesado:\n",
            "intern conflict middl east escal dramat\n"
          ]
        }
      ],
      "source": [
        "# Probar el preprocesamiento con un ejemplo\n",
        "sample_text = \"The International Conflict in the Middle East has escalated dramatically!\"\n",
        "processed_text = preprocess_text(sample_text)\n",
        "\n",
        "print(\"Texto original:\")\n",
        "print(sample_text)\n",
        "print(\"\\nTexto procesado:\")\n",
        "print(processed_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "preprocess_docs",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocess_docs",
        "outputId": "c956bcef-8ce6-427c-ef60-1d79cd8d0e01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocesando documentos arXiv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Preprocesando documentos: 100%|██████████| 50000/50000 [01:20<00:00, 623.53it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " 50000 documentos preprocesados\n",
            "Ejemplo de documento original: Calculation of prompt diphoton production cross sections at Tevatron and\n",
            "  LHC energies   A fully differential calculation in perturbative quantum chromodynamics is\n",
            "presented for the production of mas...\n",
            "\n",
            "Ejemplo de documento procesado: calcul prompt diphoton product cross section tevatron lhc energi fulli differenti calcul perturb quantum chromodynam present product massiv photon pair hadron collid next lead order perturb contribut ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Preprocesar todos los documentos de arXiv\n",
        "print(\"Preprocesando documentos arXiv...\")\n",
        "\n",
        "documents = []\n",
        "doc_ids = []\n",
        "processed_docs = []\n",
        "\n",
        "for doc in tqdm(arxiv_data, desc=\"Preprocesando documentos\"):\n",
        "    doc_ids.append(doc['id'])\n",
        "    # Combinar título y abstract para mejor búsqueda\n",
        "    full_text = f\"{doc['title']} {doc['abstract']}\"\n",
        "    documents.append(full_text)\n",
        "    processed_docs.append(preprocess_text(full_text))\n",
        "\n",
        "print(f\"\\n {len(documents)} documentos preprocesados\")\n",
        "print(f\"Ejemplo de documento original: {documents[0][:200]}...\")\n",
        "print(f\"\\nEjemplo de documento procesado: {processed_docs[0][:200]}...\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear queries sintéticas basadas en categorías de arXiv\n",
        "print(\"CREANDO QUERIES SINTÉTICAS\")\n",
        "\n",
        "class Query:\n",
        "    def __init__(self, query_id, title, description, narrative):\n",
        "        self.query_id = query_id\n",
        "        self.title = title\n",
        "        self.description = description\n",
        "        self.narrative = narrative\n",
        "\n",
        "queries_list = [\n",
        "    Query(\"Q1\", \"machine learning algorithms\",\n",
        "          \"Papers about machine learning and deep learning algorithms\",\n",
        "          \"Relevant documents discuss machine learning techniques, neural networks, and AI algorithms.\"),\n",
        "    Query(\"Q2\", \"quantum computing\",\n",
        "          \"Research on quantum computing and quantum information\",\n",
        "          \"Relevant papers cover quantum algorithms, quantum mechanics applications in computing.\"),\n",
        "    Query(\"Q3\", \"computer vision\",\n",
        "          \"Papers about image processing and computer vision\",\n",
        "          \"Documents about image recognition, object detection, and visual analysis.\"),\n",
        "    Query(\"Q4\", \"natural language processing\",\n",
        "          \"NLP and text analysis research\",\n",
        "          \"Papers covering text mining, language models, and computational linguistics.\"),\n",
        "    Query(\"Q5\", \"astrophysics cosmology\",\n",
        "          \"Research in astrophysics and cosmology\",\n",
        "          \"Documents about stars, galaxies, dark matter, and universe structure.\"),\n",
        "    Query(\"Q6\", \"molecular biology genetics\",\n",
        "          \"Papers on molecular biology and genetics\",\n",
        "          \"Research on DNA, RNA, proteins, and genetic mechanisms.\"),\n",
        "    Query(\"Q7\", \"climate change models\",\n",
        "          \"Climate modeling and environmental science\",\n",
        "          \"Papers about climate prediction, global warming, and environmental impacts.\"),\n",
        "    Query(\"Q8\", \"cryptography security\",\n",
        "          \"Cryptographic methods and security protocols\",\n",
        "          \"Documents covering encryption, security protocols, and data protection.\"),\n",
        "    Query(\"Q9\", \"robotics automation\",\n",
        "          \"Research on robotics and autonomous systems\",\n",
        "          \"Papers about robot control, autonomous navigation, and mechatronics.\"),\n",
        "    Query(\"Q10\", \"renewable energy\",\n",
        "          \"Studies on renewable energy sources\",\n",
        "          \"Documents about solar, wind, and sustainable energy technologies.\")\n",
        "]\n",
        "\n",
        "# Mostrar ejemplos\n",
        "for i, query in enumerate(queries_list[:5]):\n",
        "    print(f\"Query ID: {query.query_id}\")\n",
        "    print(f\"Title: {query.title}\")\n",
        "    print(f\"Description: {query.description}\")\n",
        "    print(f\"Narrative: {query.narrative}\")\n",
        "    print(\"-\" * 80 + \"\\n\")\n",
        "\n",
        "print(f\"✓ Total de queries creadas: {len(queries_list)}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j-62U9nn08I",
        "outputId": "70eb0a59-83cf-4334-c16d-1da6cce56d57"
      },
      "id": "0j-62U9nn08I",
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CREANDO QUERIES SINTÉTICAS\n",
            "Query ID: Q1\n",
            "Title: machine learning algorithms\n",
            "Description: Papers about machine learning and deep learning algorithms\n",
            "Narrative: Relevant documents discuss machine learning techniques, neural networks, and AI algorithms.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query ID: Q2\n",
            "Title: quantum computing\n",
            "Description: Research on quantum computing and quantum information\n",
            "Narrative: Relevant papers cover quantum algorithms, quantum mechanics applications in computing.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query ID: Q3\n",
            "Title: computer vision\n",
            "Description: Papers about image processing and computer vision\n",
            "Narrative: Documents about image recognition, object detection, and visual analysis.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query ID: Q4\n",
            "Title: natural language processing\n",
            "Description: NLP and text analysis research\n",
            "Narrative: Papers covering text mining, language models, and computational linguistics.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Query ID: Q5\n",
            "Title: astrophysics cosmology\n",
            "Description: Research in astrophysics and cosmology\n",
            "Narrative: Documents about stars, galaxies, dark matter, and universe structure.\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "✓ Total de queries creadas: 10\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear qrels sintéticos basados en categorías\n",
        "from collections import defaultdict\n",
        "\n",
        "print(\"GENERANDO QRELS SINTÉTICOS\")\n",
        "\n",
        "qrels_dict = defaultdict(dict)\n",
        "\n",
        "# Mapeo de queries a categorías relevantes de arXiv\n",
        "query_category_map = {\n",
        "    \"Q1\": [\"cs.LG\", \"cs.AI\", \"stat.ML\"],          # machine learning\n",
        "    \"Q2\": [\"quant-ph\", \"cs.ET\"],                   # quantum computing\n",
        "    \"Q3\": [\"cs.CV\"],                               # computer vision\n",
        "    \"Q4\": [\"cs.CL\", \"cs.AI\"],                      # NLP\n",
        "    \"Q5\": [\"astro-ph\"],                            # astrophysics\n",
        "    \"Q6\": [\"q-bio.GN\", \"q-bio.MN\"],               # genetics\n",
        "    \"Q7\": [\"physics.ao-ph\", \"physics.geo-ph\"],    # climate\n",
        "    \"Q8\": [\"cs.CR\"],                               # cryptography\n",
        "    \"Q9\": [\"cs.RO\"],                               # robotics\n",
        "    \"Q10\": [\"physics.soc-ph\", \"eess.SY\"],         # renewable energy\n",
        "}\n",
        "\n",
        "# Asignar relevancia basada en categorías\n",
        "print(\"Asignando relevancia a documentos...\")\n",
        "for doc in tqdm(arxiv_data, desc=\"Generando qrels\"):\n",
        "    doc_categories = doc['categories'].split()\n",
        "\n",
        "    for query_id, relevant_cats in query_category_map.items():\n",
        "        relevance = 0\n",
        "        for cat in doc_categories:\n",
        "            # Coincidencia exacta de categoría = altamente relevante\n",
        "            if any(cat.startswith(rel_cat) for rel_cat in relevant_cats):\n",
        "                relevance = 2\n",
        "                break\n",
        "            # Coincidencia de categoría principal = parcialmente relevante\n",
        "            elif any(cat.split('.')[0] == rel_cat.split('.')[0] for rel_cat in relevant_cats):\n",
        "                relevance = 1\n",
        "\n",
        "        if relevance > 0:\n",
        "            qrels_dict[query_id][doc['id']] = relevance\n",
        "\n",
        "# Mostrar estadísticas\n",
        "print(\"ESTADÍSTICAS DE QRELS\")\n",
        "for query_id in sorted(qrels_dict.keys()):\n",
        "    num_relevant = len(qrels_dict[query_id])\n",
        "    num_highly_relevant = sum(1 for rel in qrels_dict[query_id].values() if rel == 2)\n",
        "    print(f\"{query_id}: {num_relevant} docs relevantes ({num_highly_relevant} altamente relevantes)\")\n",
        "\n",
        "print(f\"\\nTotal de pares query-doc relevantes: {sum(len(docs) for docs in qrels_dict.values())}\")\n",
        "print(f\"Queries con documentos relevantes: {len(qrels_dict)}\")\n",
        "\n",
        "# Mostrar algunos ejemplos\n",
        "print(\"EJEMPLOS DE QRELS\")\n",
        "\n",
        "count = 0\n",
        "for query_id, docs in qrels_dict.items():\n",
        "    for doc_id, relevance in list(docs.items())[:2]:\n",
        "        print(f\"Query: {query_id} | Doc: {doc_id} | Relevance: {relevance}\")\n",
        "        count += 1\n",
        "        if count >= 10:\n",
        "            break\n",
        "    if count >= 10:\n",
        "        break\n",
        "\n",
        "print(\"\\nQrels generados exitosamente\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0pOzji2cn1R_",
        "outputId": "e804c03b-f2c9-4b23-a32b-724fdbe06c37"
      },
      "id": "0pOzji2cn1R_",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GENERANDO QRELS SINTÉTICOS\n",
            "Asignando relevancia a documentos...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Generando qrels: 100%|██████████| 50000/50000 [00:02<00:00, 20488.98it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ESTADÍSTICAS DE QRELS\n",
            "Q1: 3474 docs relevantes (297 altamente relevantes)\n",
            "Q10: 4007 docs relevantes (523 altamente relevantes)\n",
            "Q2: 5797 docs relevantes (3155 altamente relevantes)\n",
            "Q3: 2689 docs relevantes (58 altamente relevantes)\n",
            "Q4: 2689 docs relevantes (236 altamente relevantes)\n",
            "Q5: 10272 docs relevantes (10272 altamente relevantes)\n",
            "Q6: 846 docs relevantes (192 altamente relevantes)\n",
            "Q7: 4007 docs relevantes (219 altamente relevantes)\n",
            "Q8: 2689 docs relevantes (145 altamente relevantes)\n",
            "Q9: 2689 docs relevantes (97 altamente relevantes)\n",
            "\n",
            "Total de pares query-doc relevantes: 39159\n",
            "Queries con documentos relevantes: 10\n",
            "EJEMPLOS DE QRELS\n",
            "Query: Q1 | Doc: 0704.0002 | Relevance: 1\n",
            "Query: Q1 | Doc: 0704.0046 | Relevance: 1\n",
            "Query: Q2 | Doc: 0704.0002 | Relevance: 1\n",
            "Query: Q2 | Doc: 0704.0034 | Relevance: 2\n",
            "Query: Q3 | Doc: 0704.0002 | Relevance: 1\n",
            "Query: Q3 | Doc: 0704.0046 | Relevance: 1\n",
            "Query: Q4 | Doc: 0704.0002 | Relevance: 1\n",
            "Query: Q4 | Doc: 0704.0046 | Relevance: 1\n",
            "Query: Q8 | Doc: 0704.0002 | Relevance: 1\n",
            "Query: Q8 | Doc: 0704.0046 | Relevance: 1\n",
            "\n",
            "Qrels generados exitosamente\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "preprocess_queries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "preprocess_queries",
        "outputId": "c32e72ba-7f10-4bc6-9b0f-bada1f63d193"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocesando queries...\n",
            "\n",
            "10 queries preprocesadas\n",
            "\n",
            "Ejemplo de query original: machine learning algorithms Papers about machine learning and deep learning algorithms\n",
            "Ejemplo de query procesada: machin learn algorithm paper machin learn deep learn algorithm\n"
          ]
        }
      ],
      "source": [
        "# Preprocesar las queries\n",
        "print(\"Preprocesando queries...\")\n",
        "\n",
        "query_ids = []\n",
        "query_texts = []\n",
        "processed_queries = []\n",
        "\n",
        "for query in queries_list:\n",
        "    query_ids.append(query.query_id)\n",
        "    # Combinar title y description para mejor recuperación\n",
        "    full_query = f\"{query.title} {query.description}\"\n",
        "    query_texts.append(full_query)\n",
        "    processed_queries.append(preprocess_text(full_query))\n",
        "\n",
        "print(f\"\\n{len(query_texts)} queries preprocesadas\")\n",
        "print(f\"\\nEjemplo de query original: {query_texts[0]}\")\n",
        "print(f\"Ejemplo de query procesada: {processed_queries[0]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec3_md",
      "metadata": {
        "id": "sec3_md"
      },
      "source": [
        "## 4. Generación de Embeddings\n",
        "\n",
        "Sentence-BERT (modelo pre-entrenado) para generar embeddings semánticos de:\n",
        "- Documentos del corpus\n",
        "- Queries de búsqueda\n",
        "\n",
        "Modelo usado: all-MiniLM-L6-v2 (rápido y eficiente para embeddings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "load_model",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 615,
          "referenced_widgets": [
            "508d1aa601744a99a8a8bc7d81ac952f",
            "a5de2af806504f94863c23c5eabe3918",
            "7e4c51eb45b24d47a8204a8b603766ad",
            "5095544bf5ff49ccb62917171621c302",
            "b253db256213470b90f3ff5883b54f96",
            "20e43dd95b614a38b95c5e680cd052a5",
            "2ab5fca83ab94e2da599230cf316db97",
            "0139a787c31a49b4ad83c6d05faaf14d",
            "7d7ff1e965d5401c9ca9de2696914568",
            "a9eb0370517b4199a0505a47b2cd6b5d",
            "598a6e1a106f4a179db82edf566890cd",
            "8849f6b583c04e9588428c0b17d39d5e",
            "c225dc539c2c4e36bd65f2211d0f4ece",
            "4c7ae2ea16ce4977a67e20ad11447343",
            "cd6519873f464245a49f04c384e62e7c",
            "a11a6414d7f6485d86a5b84c6dbe748c",
            "858480acd066467d86cee266f177aed8",
            "4f1fec100e784aa0b3a4a79559a4d1bc",
            "ef72ed0403964d3eb8dd00fd845d2819",
            "5b60fd25ee0d49e3bd5528000c505bbd",
            "b4f25458330e472ca23cfd09d2e0b1c2",
            "6b7ef7fdd7a84beb9d82d6bdc7a233b5",
            "ec58109386b149e5ab2bb8eb32318c76",
            "c2c42338f3334e9e980afea294932d7f",
            "8f1a33afce6e4a18a726018bb6067f15",
            "5ed2a9b517954868bd861303ee246206",
            "7357f4c8fca44e5797f4acd56210ee6a",
            "5adb095a45b54cec9c82719093a85520",
            "cb98eac6e61e4c9d95aa1086f20c0118",
            "1db76d1cd108465d87d2a0ea2f30d70c",
            "5848f454c03a40a391d724fc032c50fa",
            "2b42c91b4c334806a96e9da66387923d",
            "e24891e1567f43aba8a99f6c1d415831",
            "dc4e439f98334ab8a1ff4db02178b640",
            "36907fd24f5c4065ae1d2a65e948d407",
            "4d54c14f792d467ab42e56193c3d6d99",
            "9a6a98dd716d4d47b21c7d1183c26fb6",
            "d99cf81f6d484dd2b06917827a0efe54",
            "c150b2ba071e478d9ff74afe5e855229",
            "270c2f7a2b0147ada6a7440fc09be210",
            "b25cb8bfde5345779d187cfda7a27860",
            "9c390bb53f064ab9b3f20f2cbe797387",
            "a6421b81ba7b4fa8a51f7697830db267",
            "7eb173e4621a48d7b595d9299df3e159",
            "8752bcd6701943ce90a42e448779f841",
            "f3ad021a1b914889ae443d8d15533b6b",
            "a4f53074e559421191ce06f3dc6b537d",
            "3460a3eab745477993af07baa752e176",
            "2a31cb17c0a44e618a0fb03b5fed4286",
            "12f39173f732470291386e31367b79a6",
            "7f6717fc59c14744a044af0fa4dbd9ce",
            "af39b5d2d7b64a219d75235dc3c21194",
            "d2d5d0c3abc64b42932f7248f7d2235e",
            "d9b45af54c514f729a6f3d4e0a96d5e5",
            "ab1e4eba54024fcf94d56c2472a6e630",
            "5a61f43a4a2c4aeb98a91c2b16e0ae2d",
            "c7c92df02fda4fc8a08d5c98e00c9548",
            "353103667ee344bd858adec90fdc4df9",
            "dbf1418617c24274963ccdb622934956",
            "4254bc223fc84fffac1b2a101f925551",
            "c4286724a35345778c81cf4700acba0e",
            "c17b551a7cb44099abfc03aeebb9d55d",
            "82382f67dae84bd8a8a9e0901fa3f2c9",
            "63a29b82ad9646c78ce76cb740026721",
            "e9280e1f9c7a4d4298f655198b4093e5",
            "06aad9fe3ed94bb48fff374df69593a3",
            "b108e9a05f484044a4cb504e43eb8479",
            "acbe3c4f62ef4c9d9d487157c621fab2",
            "1ce526f52bb741b8adc22043f00f426c",
            "8bf4c130af524076a1b68fe8d7af64f7",
            "5e822b425fad446699bb5e74563322a5",
            "588495b05cf54a1b89437ef28c0a97ca",
            "80a9690e1fde474e9707500103889e2f",
            "163641d0a58f40809b04dff9195b0d12",
            "07f106d37916421598dafff7e107fe6b",
            "1670a82015aa40c296f6e74662540c21",
            "2898d8c2f0764498b08f570d7ed0626e",
            "186b086b1ba64a07adc99149870c4e9f",
            "f7afdfcc15844c1ba0d70c02339c730f",
            "e8e24ed03a4447c390d1b5b4497dc4dc",
            "e79d06c550b546bb90cddc27ff4e1f8f",
            "68b95b7cbb6c4778a5dbb218c9fd7984",
            "34fd2a6e58874fde854b5c73350e87b0",
            "d41f516a438c48ccb50bd0851321135b",
            "5c48ca3fa9d74318a991b23c3f4ea10c",
            "6bac0d8e5dea4623bd57192098545781",
            "72ee46e2852c4a6ea6dd10672f777128",
            "767d531d9afd47949ee373e0922985e1",
            "70d099d181bb4d4c80427bb4a07b3f57",
            "a1cb976b268c42fe941ae061dfe702b9",
            "b01fa4f0a9594b35a530e94e0e96ee23",
            "9efd06e6eafe4374b5517ffbf0cadc83",
            "751f9153e73449f7808d7b7f04827fd3",
            "2c83a54c2914408a9477e72e0d5382b5",
            "a26eb6d270134f628f3b4da27d26f923",
            "d88a04d0326c409cb1266d9fe6cd09fa",
            "cdf5086704194e96a9f4d2f9c733133f",
            "b9115ae1abf344548ba98bf307e52f97",
            "95f671c4c4e14bd78d3d12b1caeac2a3",
            "f78b4497ecb74a1197aa64ea4cb81bee",
            "81279015ce9444bc94fa04d6fa97600a",
            "f6b1e9be689c419995a62713e7d5c902",
            "2aab87464716407ba1c13c6cb4d2301f",
            "c97ab87721e94aa892aa0fcc8e97530c",
            "99c5cc404aa7426f87db8cbb90a3227f",
            "d0696bd5109e43a9adc81e427da6bc39",
            "45d133e0a98e4f6bb835774cfb8993f3",
            "69cd7f0d4fd442559b1bfad834b610d4",
            "6c0b50f4e7eb481a9af561c32b03c95f",
            "327f081ea273478c8ed2535629315b31",
            "e426435eb38b4a7dba7b1ead29f05304",
            "49a045650520434ab0636f7da1d3df3a",
            "7e22e308f794498b9346e12a4ee4a5c3",
            "64516c8da95c4b7bba2e114ebc5bb635",
            "a27475c467e44df0aa454ddf6ecca0fa",
            "b6226200541641b887a298d1b24b0739",
            "a650e0d9ac6649edb5451e07aefabd6b",
            "cd42b30b4d9e42808b57640485dc4bd4",
            "1c5e43c6460e48ab93532093edc9cafd",
            "7b765ccce13147eaa02714fa530ee3ce",
            "7360954934254d98b2eb6dfcf28cfb0f"
          ]
        },
        "id": "load_model",
        "outputId": "7933197a-261f-44c2-e493-779c014d7267"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo de embeddings...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "508d1aa601744a99a8a8bc7d81ac952f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8849f6b583c04e9588428c0b17d39d5e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ec58109386b149e5ab2bb8eb32318c76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dc4e439f98334ab8a1ff4db02178b640"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8752bcd6701943ce90a42e448779f841"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a61f43a4a2c4aeb98a91c2b16e0ae2d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b108e9a05f484044a4cb504e43eb8479"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "186b086b1ba64a07adc99149870c4e9f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "70d099d181bb4d4c80427bb4a07b3f57"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f78b4497ecb74a1197aa64ea4cb81bee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e426435eb38b4a7dba7b1ead29f05304"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Modelo cargado: SentenceTransformer(\n",
            "  (0): Transformer({'max_seq_length': 256, 'do_lower_case': False, 'architecture': 'BertModel'})\n",
            "  (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
            "  (2): Normalize()\n",
            ")\n",
            "Dimensión de embeddings: 384\n"
          ]
        }
      ],
      "source": [
        "# Cargar el modelo de embeddings\n",
        "print(\"Cargando modelo de embeddings...\")\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "print(f\"✓ Modelo cargado: {embedding_model}\")\n",
        "print(f\"Dimensión de embeddings: {embedding_model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "generate_doc_embeddings",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136,
          "referenced_widgets": [
            "d9f810d4e5504aae9983171ed83624ec",
            "9d5299bbe4d84fd4b21893d7ed4363b0",
            "30f26a5e674c4cd9a44b8cab82b7f229",
            "027a8127a02f4995b238962cb7f4a785",
            "6a61c376ed354e35b85534f57154eee2",
            "1376fe499bb6455685cbbd6ae971a81a",
            "17ccff80dc844cf6abf0ebe6a9f1f6f5",
            "b40e36e35a834952a908c01a0fd6359a",
            "89922bed75434bf28b7bba215d34b518",
            "5dab3dafe2b1409a8906923400f600cd",
            "90ca1a014b69468bb588819fb663cd78"
          ]
        },
        "id": "generate_doc_embeddings",
        "outputId": "ee0ea29c-4ce7-491e-a02c-da29600351f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando embeddings para documentos...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1563 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9f810d4e5504aae9983171ed83624ec"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Embeddings de documentos generados\n",
            "Shape: (50000, 384)\n",
            "Tipo de datos: float32\n"
          ]
        }
      ],
      "source": [
        "# Generar embeddings para los documentos\n",
        "print(\"Generando embeddings para documentos...\")\n",
        "# Usar los textos originales para mejores embeddings semánticos\n",
        "batch_size = 32\n",
        "doc_embeddings = embedding_model.encode(\n",
        "    documents,\n",
        "    batch_size=batch_size,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "print(f\"\\nEmbeddings de documentos generados\")\n",
        "print(f\"Shape: {doc_embeddings.shape}\")\n",
        "print(f\"Tipo de datos: {doc_embeddings.dtype}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "generate_query_embeddings",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 118,
          "referenced_widgets": [
            "cd059e6e618d439d909b54490dc52861",
            "52d3f1a1fc7749eea81947ee6daf702d",
            "e2095df82f9a4115867f2b9ca3b7b869",
            "cc5483ac510d4d6b865ac8e6750e6846",
            "243f8d74198c4ae7b221ea45f48ddf39",
            "4550b6afbd2e455daea153a169d558eb",
            "3c83e4b0aea04d85ba57439115556192",
            "a5a1f1d969484599bdd91f3f0fe08e5c",
            "c0bc4941c7f04d219ff47450ed544304",
            "10c6166d60624789a4f09b8ec038b4b0",
            "9753d23dba5a4a5888b133a38fbca091"
          ]
        },
        "id": "generate_query_embeddings",
        "outputId": "29c77ae2-9162-4e47-9e4a-f4aca9b1df19"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generando embeddings para queries...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd059e6e618d439d909b54490dc52861"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Embeddings de queries generados\n",
            "Shape: (10, 384)\n"
          ]
        }
      ],
      "source": [
        "# Generar embeddings para las queries\n",
        "print(\"Generando embeddings para queries...\")\n",
        "\n",
        "query_embeddings = embedding_model.encode(\n",
        "    query_texts,\n",
        "    show_progress_bar=True,\n",
        "    convert_to_numpy=True\n",
        ")\n",
        "\n",
        "print(f\"\\n Embeddings de queries generados\")\n",
        "print(f\"Shape: {query_embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec4_md",
      "metadata": {
        "id": "sec4_md"
      },
      "source": [
        "## 5. Construcción del Índice FAISS\n",
        "\n",
        "Utiliza\n",
        "**FAISS** (Facebook AI Similarity Search) para crear un índice vectorial eficiente que permita:\n",
        "- Búsqueda rápida de vecinos más cercanos\n",
        "- Recuperación de top-k documentos candidatos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "build_faiss_index",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "build_faiss_index",
        "outputId": "d9d507e3-31b2-4ef0-aae7-d076adc43474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Construyendo índice FAISS...\n",
            "\n",
            "Índice FAISS construido\n",
            "Total de vectores en el índice: 50000\n",
            "Dimensión: 384\n"
          ]
        }
      ],
      "source": [
        "# Construir índice FAISS\n",
        "print(\"Construyendo índice FAISS...\")\n",
        "\n",
        "# Dimensión de los embeddings\n",
        "dimension = doc_embeddings.shape[1]\n",
        "\n",
        "# Crear índice flat (búsqueda exacta por fuerza bruta - preciso pero más lento)\n",
        "# Para datasets grandes, se puede usar IndexIVFFlat para mayor velocidad\n",
        "index = faiss.IndexFlatIP(dimension)  # IP = Inner Product (similar a cosine similarity)\n",
        "\n",
        "# Normalizar embeddings para que el producto interno sea equivalente a similitud coseno\n",
        "faiss.normalize_L2(doc_embeddings)\n",
        "\n",
        "# Agregar vectores al índice\n",
        "index.add(doc_embeddings)\n",
        "\n",
        "print(f\"\\nÍndice FAISS construido\")\n",
        "print(f\"Total de vectores en el índice: {index.ntotal}\")\n",
        "print(f\"Dimensión: {dimension}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec5_md",
      "metadata": {
        "id": "sec5_md"
      },
      "source": [
        "## 6. Recuperación Inicial (First-Stage Retrieval)\n",
        "\n",
        "Implementar la búsqueda vectorial para recuperar los **top-k documentos candidatos** para cada query."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "initial_retrieval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "initial_retrieval",
        "outputId": "e4c53678-fd2b-4d28-c11e-e728e54ab42c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función de recuperación inicial definida\n"
          ]
        }
      ],
      "source": [
        "# Función para recuperación inicial\n",
        "def retrieve_initial_candidates(query_embedding, k=100):\n",
        "#    Recupera los top-k documentos más similares a la query.\n",
        "    # Normalizar la query\n",
        "    query_embedding = query_embedding.reshape(1, -1).astype('float32')\n",
        "    faiss.normalize_L2(query_embedding)\n",
        "\n",
        "    # Buscar los k vecinos más cercanos\n",
        "    distances, indices = index.search(query_embedding, k)\n",
        "\n",
        "    return distances[0], indices[0]\n",
        "\n",
        "print(\"Función de recuperación inicial definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "test_initial_retrieval",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_initial_retrieval",
        "outputId": "c28382a9-150a-4e49-8863-4f0188344980"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query de prueba: 'machine learning algorithms Papers about machine learning and deep learning algorithms'\n",
            "Query ID: Q1\n",
            "\n",
            "TOP 10 DOCUMENTOS RECUPERADOS (Recuperación Inicial):\n",
            "================================================================================\n",
            "\n",
            "Rank 1: Score=0.4051\n",
            "Doc ID: 0708.2321\n",
            "Texto: Fast learning rates for plug-in classifiers   It has been recently shown that, under the margin (or low noise) assumption,\n",
            "there exist classifiers attaining fast rates of convergence of the excess Bay...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 2: Score=0.3910\n",
            "Doc ID: 0707.0303\n",
            "Texto: Learning from dependent observations   In most papers establishing consistency for learning algorithms it is assumed\n",
            "that the observations used for training are realizations of an i.i.d. process.\n",
            "In t...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 3: Score=0.3900\n",
            "Doc ID: 0709.1201\n",
            "Texto: On the Proof Complexity of Deep Inference   We obtain two results about the proof complexity of deep inference: 1)\n",
            "deep-inference proof systems are as powerful as Frege ones, even when both are\n",
            "extend...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 4: Score=0.3865\n",
            "Doc ID: 0712.4126\n",
            "Texto: TRUST-TECH based Methods for Optimization and Learning   Many problems that arise in machine learning domain deal with nonlinearity\n",
            "and quite often demand users to obtain global optimal solutions rath...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 5: Score=0.3793\n",
            "Doc ID: 0712.4099\n",
            "Texto: Digital Ecosystems: Optimisation by a Distributed Intelligence   Can intelligence optimise Digital Ecosystems? How could a distributed\n",
            "intelligence interact with the ecosystem dynamics? Can the softwa...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 6: Score=0.3704\n",
            "Doc ID: 0708.1838\n",
            "Texto: Fast rates for support vector machines using Gaussian kernels   For binary classification we establish learning rates up to the order of\n",
            "$n^{-1}$ for support vector machines (SVMs) with hinge loss and...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 7: Score=0.3681\n",
            "Doc ID: 0704.3780\n",
            "Texto: Stochastic Optimization Algorithms   When looking for a solution, deterministic methods have the enormous\n",
            "advantage that they do find global optima. Unfortunately, they are very\n",
            "CPU-intensive, and are...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 8: Score=0.3667\n",
            "Doc ID: 0708.2377\n",
            "Texto: Online Learning in Discrete Hidden Markov Models   We present and analyse three online algorithms for learning in discrete\n",
            "Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 9: Score=0.3646\n",
            "Doc ID: 0712.1027\n",
            "Texto: Kernels and Ensembles: Perspectives on Statistical Learning   Since their emergence in the 1990's, the support vector machine and the\n",
            "AdaBoost algorithm have spawned a wave of research in statistical ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 10: Score=0.3592\n",
            "Doc ID: 0712.3329\n",
            "Texto: Universal Intelligence: A Definition of Machine Intelligence   A fundamental problem in artificial intelligence is that nobody really knows\n",
            "what intelligence is. The problem is especially acute when w...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Probar recuperación inicial con una query de ejemplo\n",
        "test_query_idx = 0\n",
        "test_query = query_texts[test_query_idx]\n",
        "test_query_id = query_ids[test_query_idx]\n",
        "\n",
        "print(f\"Query de prueba: '{test_query}'\")\n",
        "print(f\"Query ID: {test_query_id}\")\n",
        "\n",
        "# Recuperar top-10 documentos\n",
        "scores, retrieved_indices = retrieve_initial_candidates(query_embeddings[test_query_idx], k=10)\n",
        "\n",
        "print(\"\\nTOP 10 DOCUMENTOS RECUPERADOS (Recuperación Inicial):\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "for rank, (idx, score) in enumerate(zip(retrieved_indices, scores), 1):\n",
        "    doc_id = doc_ids[idx]\n",
        "    doc_text = documents[idx][:200]\n",
        "    print(f\"\\nRank {rank}: Score={score:.4f}\")\n",
        "    print(f\"Doc ID: {doc_id}\")\n",
        "    print(f\"Texto: {doc_text}...\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "retrieve_all_queries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "retrieve_all_queries",
        "outputId": "0d061e4b-17e7-467b-f8ef-fbe51d99a7aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 10/10 [00:00<00:00, 52.61it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Recuperación inicial completada para 10 queries\n",
            "Promedio de documentos por query: 100.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Recuperar candidatos para todas las queries\n",
        "K_CANDIDATES = 100  # Número de candidatos a recuperar por query\n",
        "initial_results = {}\n",
        "\n",
        "for i, (query_id, query_emb) in enumerate(tqdm(zip(query_ids, query_embeddings), total=len(query_ids))):\n",
        "    scores, indices = retrieve_initial_candidates(query_emb, k=K_CANDIDATES)\n",
        "\n",
        "    # Guardar resultados\n",
        "    initial_results[query_id] = [\n",
        "        (doc_ids[idx], float(score))\n",
        "        for idx, score in zip(indices, scores)\n",
        "    ]\n",
        "\n",
        "print(f\"\\nRecuperación inicial completada para {len(initial_results)} queries\")\n",
        "print(f\"Promedio de documentos por query: {np.mean([len(docs) for docs in initial_results.values()]):.1f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec6_md",
      "metadata": {
        "id": "sec6_md"
      },
      "source": [
        "## 7. Re-ranking de Resultados\n",
        "\n",
        "Implementar un **cross-encoder** para re-rankear los documentos candidatos.\n",
        "\n",
        "Los cross-encoders son más precisos que los bi-encoders porque procesan query y documento juntos, pero son más lentos (por eso solo los usamos en los top-k candidatos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "load_reranker",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "5d819e8f138b4c948b9a1460d3516a76",
            "bd1d5461658149b6b66c130060aead34",
            "4b2715f1e08a46938119ecc7e56d7ffb",
            "aa2d20d30d824da299e19024c12e7e87",
            "61477fd0d27e4fe586c927f2950e876f",
            "5e3b8d1647794d1498e5a137aaffda97",
            "50e47e9eed0640a79b6550d5402e032b",
            "590937741b294843b422a0a3259fd64f",
            "093f17f829b74a6ba22fa9674829245e",
            "bd08b1adfbe04468a37cf961f5c69d24",
            "2bbc652c36404f53ac84475b00c50976",
            "a9774d0b438d470c8503c8f3d7ad9d7a",
            "b5de4cdffd754904ad068378b93dea8e",
            "3dff4e36a999453b8e00c1c3e3783f1d",
            "5ec9145331d04d4b9f0b8477384b1248",
            "8054529000114bc7a43fbc35ac817ff8",
            "90c11d69109f4213b27ab5706467a66e",
            "c5ec3ac851ac41f7960bbaddf5eb65de",
            "240010c2b1d149e49d495926542a45a9",
            "827bc989a66a40c3a967611f2e78fb8f",
            "83b2a5c8ac4f4fd9a9642ccce2800ed7",
            "a2b3c6156f04485d9d66d8f32d2c731f",
            "8521c033329e46478174ce9aefd5b03e",
            "40583119870e4b729f09769bb136aba0",
            "4b418576dab748fcb349d3503b1185b6",
            "c2f660c86e8d4a448291abbd0e498f14",
            "2f0452546f4c4e62b641f9b9d7ee8f34",
            "45d7baba853f44f79b1899fce149ffb6",
            "3577c9be1aee4fb8a61fb7ffec26b039",
            "121d5d46dc4f436dbf52d35a3b185184",
            "c3470862f5db43658fdee978ce67015d",
            "d7f78be069dc471bb7189caf4584a056",
            "5c224cd7fa7f49e8afb0b36719bbcc44",
            "4a59c598a6754b16b15d184b1d6338e7",
            "8f482ecf11904f039b13823ddc9eb320",
            "c2855cd5607848649d3b3ca6412a49b1",
            "52143be185d84445a649f940afc03555",
            "8e03f91ccaf74c9d946316a701d463e5",
            "f12998593f2e4bb68f36dc1097a9f167",
            "467fc05077f14c7db925be790f8f3e0b",
            "5b06baf13a2a42b8b048355c6201b14a",
            "88e2552f76a447dc88e567a366d51095",
            "11c25bbf2cd641d7af1ae3355bfc7725",
            "3b708c930bbf433dbf23f14780f51f98",
            "8affb27ccd584166a5cb92e11b93da5c",
            "5b0d2152d14e48ff93027b5afdab8467",
            "1b16c67754864c4fb504658eb619bdbc",
            "1ae7f5e72cfa42c1bc9707085fdb45a3",
            "7a9588d4a1184113b149492d9a75b158",
            "ca3d7138130d4a3d986bd5cdf1795265",
            "5ab05ea19e234935aad0538ad87ec18d",
            "502b16fbab7540a289f139f95d2d04ed",
            "a581371a7747466eadd8728d9f3b6ea3",
            "25b790bf34624baf8b3ce5cbe2650bff",
            "75099699055745dcb6bdba12928e0da6",
            "5a165bd32c454416852efb3645968e91",
            "ea9129609fb04fde895153f23caa2ee4",
            "f32b933471b047878c37457fd5cc4e1d",
            "eaa1383c7a1a4eeca9f9711797c679c7",
            "76c791e984514d6cac6928d248536131",
            "2a26858f9cee4e1192e8726a76832d94",
            "7ea7162c46cb440198336c4a548810e0",
            "336c427e91864a95890159489e85c27a",
            "99b72de6e7b445b7824f429cadc290a0",
            "21a464fd29954d21a05abd1828a85b43",
            "f8e11887619d4d14a261107f63b68eec",
            "388da8935a8d4a2983c36162888c6bd8",
            "a68e54680dde484992a0d3412cb371be",
            "a62b1139a12f457b8c3219887183ce6f",
            "5bb8e403c0af42829838eeff998c59d7",
            "29a712e491f64f159a1fc8e9ef830117",
            "9fcf4d1b9cb54d98a3bc53df9b64f6f9",
            "f5bae8e2993246d1bd238956f3735f98",
            "30928c5020b24315870761bf171d0025",
            "bbf299cf014b40bcb49d400580c1e821",
            "c88189ff23b14603835ebdb8e8c6cf1a",
            "c99a5a6620864b94a842e17024341256"
          ]
        },
        "id": "load_reranker",
        "outputId": "cff37616-32d5-479e-f0ee-ce4617e3881c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cargando modelo de re-ranking (Cross-Encoder)...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/794 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d819e8f138b4c948b9a1460d3516a76"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a9774d0b438d470c8503c8f3d7ad9d7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer_config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8521c033329e46478174ce9aefd5b03e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "vocab.txt: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4a59c598a6754b16b15d184b1d6338e7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8affb27ccd584166a5cb92e11b93da5c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/132 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5a165bd32c454416852efb3645968e91"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "README.md: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "388da8935a8d4a2983c36162888c6bd8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Modelo de re-ranking cargado: CrossEncoder(\n",
            "  (model): BertForSequenceClassification(\n",
            "    (bert): BertModel(\n",
            "      (embeddings): BertEmbeddings(\n",
            "        (word_embeddings): Embedding(30522, 384, padding_idx=0)\n",
            "        (position_embeddings): Embedding(512, 384)\n",
            "        (token_type_embeddings): Embedding(2, 384)\n",
            "        (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (encoder): BertEncoder(\n",
            "        (layer): ModuleList(\n",
            "          (0-5): 6 x BertLayer(\n",
            "            (attention): BertAttention(\n",
            "              (self): BertSdpaSelfAttention(\n",
            "                (query): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (key): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (value): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "              (output): BertSelfOutput(\n",
            "                (dense): Linear(in_features=384, out_features=384, bias=True)\n",
            "                (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
            "                (dropout): Dropout(p=0.1, inplace=False)\n",
            "              )\n",
            "            )\n",
            "            (intermediate): BertIntermediate(\n",
            "              (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (intermediate_act_fn): GELUActivation()\n",
            "            )\n",
            "            (output): BertOutput(\n",
            "              (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
            "              (dropout): Dropout(p=0.1, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pooler): BertPooler(\n",
            "        (dense): Linear(in_features=384, out_features=384, bias=True)\n",
            "        (activation): Tanh()\n",
            "      )\n",
            "    )\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "    (classifier): Linear(in_features=384, out_features=1, bias=True)\n",
            "  )\n",
            "  (activation_fn): Identity()\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "# Cargar modelo de re-ranking\n",
        "print(\"Cargando modelo de re-ranking (Cross-Encoder)...\")\n",
        "reranker = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')\n",
        "print(f\"Modelo de re-ranking cargado: {reranker}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "rerank_function",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rerank_function",
        "outputId": "0a618250-a1b9-449d-ef5d-d6ade5cae5a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función de re-ranking definida\n"
          ]
        }
      ],
      "source": [
        "# Función de re-ranking\n",
        "def rerank_documents(query_text, candidate_doc_ids, top_k=10):\n",
        "  # Re-rankea los documentos candidatos usando un cross-encoder.\n",
        "    # Obtener textos de los documentos candidatos\n",
        "    doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
        "    candidate_texts = [documents[doc_id_to_idx[doc_id]] for doc_id in candidate_doc_ids]\n",
        "\n",
        "    # Crear pares (query, documento) para el cross-encoder\n",
        "    pairs = [[query_text, doc_text] for doc_text in candidate_texts]\n",
        "\n",
        "    # Calcular scores de relevancia\n",
        "    scores = reranker.predict(pairs)\n",
        "\n",
        "    # Ordenar por score descendente\n",
        "    ranked_results = sorted(\n",
        "        zip(candidate_doc_ids, scores),\n",
        "        key=lambda x: x[1],\n",
        "        reverse=True\n",
        "    )\n",
        "\n",
        "    return ranked_results[:top_k]\n",
        "\n",
        "print(\"Función de re-ranking definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "test_reranking",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_reranking",
        "outputId": "4f745a2d-f5db-4921-e58b-88d93fd91455"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Query de prueba: 'machine learning algorithms Papers about machine learning and deep learning algorithms'\n",
            "Aplicando re-ranking...\n",
            "\n",
            "TOP 10 DOCUMENTOS DESPUÉS DE RE-RANKING:\n",
            "\n",
            "Rank 1: Score=2.3719\n",
            "Doc ID: 0712.4126\n",
            "Texto: TRUST-TECH based Methods for Optimization and Learning   Many problems that arise in machine learning domain deal with nonlinearity\n",
            "and quite often demand users to obtain global optimal solutions rath...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 2: Score=1.4214\n",
            "Doc ID: 0707.0303\n",
            "Texto: Learning from dependent observations   In most papers establishing consistency for learning algorithms it is assumed\n",
            "that the observations used for training are realizations of an i.i.d. process.\n",
            "In t...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 3: Score=1.1026\n",
            "Doc ID: 0712.1027\n",
            "Texto: Kernels and Ensembles: Perspectives on Statistical Learning   Since their emergence in the 1990's, the support vector machine and the\n",
            "AdaBoost algorithm have spawned a wave of research in statistical ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 4: Score=1.0504\n",
            "Doc ID: 0710.5896\n",
            "Texto: Supervised Machine Learning with a Novel Pointwise Density Estimator   This article proposes a novel density estimation based algorithm for carrying\n",
            "out supervised machine learning. The proposed algor...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 5: Score=1.0442\n",
            "Doc ID: 0802.1412\n",
            "Texto: Extreme Learning Machine for land cover classification   This paper explores the potential of extreme learning machine based\n",
            "supervised classification algorithm for land cover classification. In\n",
            "compa...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 6: Score=0.6682\n",
            "Doc ID: 0710.4482\n",
            "Texto: Robust Machine Learning Applied to Terascale Astronomical Datasets   We present recent results from the Laboratory for Cosmological Data Mining\n",
            "(http://lcdm.astro.uiuc.edu) at the National Center for ...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 7: Score=0.3239\n",
            "Doc ID: 0704.1274\n",
            "Texto: Parametric Learning and Monte Carlo Optimization   This paper uncovers and explores the close relationship between Monte Carlo\n",
            "Optimization of a parametrized integral (MCO), Parametric machine-Learnin...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 8: Score=-1.8656\n",
            "Doc ID: 0711.1401\n",
            "Texto: Towards a Sound Theory of Adaptation for the Simple Genetic Algorithm   The pace of progress in the fields of Evolutionary Computation and Machine\n",
            "Learning is currently limited -- in the former field,...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 9: Score=-2.7306\n",
            "Doc ID: 0708.2377\n",
            "Texto: Online Learning in Discrete Hidden Markov Models   We present and analyse three online algorithms for learning in discrete\n",
            "Hidden Markov Models (HMMs) and compare them with the Baldi-Chauvin Algorithm...\n",
            "--------------------------------------------------------------------------------\n",
            "\n",
            "Rank 10: Score=-2.9237\n",
            "Doc ID: 0709.2760\n",
            "Texto: Supervised Machine Learning with a Novel Kernel Density Estimator   In recent years, kernel density estimation has been exploited by computer\n",
            "scientists to model machine learning problems. The kernel ...\n",
            "--------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Probar re-ranking con la misma query de ejemplo\n",
        "print(f\"Query de prueba: '{test_query}'\")\n",
        "\n",
        "# Obtener candidatos iniciales (top-50 para re-ranking)\n",
        "initial_candidate_ids = [doc_id for doc_id, _ in initial_results[test_query_id][:50]]\n",
        "\n",
        "# Re-rankear\n",
        "print(\"Aplicando re-ranking...\")\n",
        "reranked_results = rerank_documents(test_query, initial_candidate_ids, top_k=10)\n",
        "print(\"\\nTOP 10 DOCUMENTOS DESPUÉS DE RE-RANKING:\")\n",
        "\n",
        "doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
        "for rank, (doc_id, score) in enumerate(reranked_results, 1):\n",
        "    idx = doc_id_to_idx[doc_id]\n",
        "    doc_text = documents[idx][:200]\n",
        "    print(f\"\\nRank {rank}: Score={score:.4f}\")\n",
        "    print(f\"Doc ID: {doc_id}\")\n",
        "    print(f\"Texto: {doc_text}...\")\n",
        "    print(\"-\" * 80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "rerank_all_queries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rerank_all_queries",
        "outputId": "d0a5ec60-5826-4739-d9f5-89127befe14b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Aplicando re-ranking a todas las queries...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Re-ranking: 100%|██████████| 10/10 [00:02<00:00,  4.32it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Re-ranking completado para 10 queries\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Re-rankear para todas las queries\n",
        "print(\"Aplicando re-ranking a todas las queries...\")\n",
        "\n",
        "RERANK_TOP_K = 50  # Cuántos candidatos re-rankear por query\n",
        "FINAL_TOP_K = 10   # Cuántos documentos finales retornar\n",
        "\n",
        "reranked_results = {}\n",
        "\n",
        "for query_id, query_text in tqdm(zip(query_ids, query_texts), total=len(query_ids), desc=\"Re-ranking\"):\n",
        "    # Obtener top candidatos iniciales\n",
        "    initial_candidates = [doc_id for doc_id, _ in initial_results[query_id][:RERANK_TOP_K]]\n",
        "\n",
        "    # Re-rankear\n",
        "    reranked = rerank_documents(query_text, initial_candidates, top_k=FINAL_TOP_K)\n",
        "    reranked_results[query_id] = reranked\n",
        "\n",
        "print(f\"\\nRe-ranking completado para {len(reranked_results)} queries\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec7_md",
      "metadata": {
        "id": "sec7_md"
      },
      "source": [
        "## 8. Simulación de Consultas\n",
        "\n",
        "Ejecuta múltiples consultas y mostramos los resultados antes y después del re-ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "simulate_queries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "simulate_queries",
        "outputId": "b2a39f53-2ed6-4c5f-9fc2-535d6f77a944"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Función de visualización definida\n"
          ]
        }
      ],
      "source": [
        "# Función para visualizar resultados de una query\n",
        "def display_query_results(query_id, query_text, initial_results_list, reranked_results_list, top_n=5):\n",
        "    print(\"\\n\" + \"=\"*100)\n",
        "    print(f\"QUERY ID: {query_id}\")\n",
        "    print(f\"QUERY TEXT: {query_text}\")\n",
        "    print(\"=\"*100)\n",
        "\n",
        "    doc_id_to_idx = {doc_id: idx for idx, doc_id in enumerate(doc_ids)}\n",
        "\n",
        "    # Resultados iniciales\n",
        "    print(f\"\\n{'RECUPERACIÓN INICIAL (Top-' + str(top_n) + ')':^100}\")\n",
        "    print(\"-\" * 100)\n",
        "    for rank, (doc_id, score) in enumerate(initial_results_list[:top_n], 1):\n",
        "        if doc_id in doc_id_to_idx:\n",
        "            idx = doc_id_to_idx[doc_id]\n",
        "            doc_snippet = documents[idx][:150]\n",
        "            print(f\"\\n[{rank}] Score: {score:.4f} | Doc ID: {doc_id}\")\n",
        "            print(f\"    {doc_snippet}...\")\n",
        "\n",
        "    # Resultados re-rankeados\n",
        "    print(f\"\\n\\n{'DESPUÉS DE RE-RANKING (Top-' + str(top_n) + ')':^100}\")\n",
        "    print(\"-\" * 100)\n",
        "    for rank, (doc_id, score) in enumerate(reranked_results_list[:top_n], 1):\n",
        "        if doc_id in doc_id_to_idx:\n",
        "            idx = doc_id_to_idx[doc_id]\n",
        "            doc_snippet = documents[idx][:150]\n",
        "            print(f\"\\n[{rank}] Score: {score:.4f} | Doc ID: {doc_id}\")\n",
        "            print(f\"    {doc_snippet}...\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"Función de visualización definida\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "show_sample_queries",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "show_sample_queries",
        "outputId": "1283305b-7216-45d6-826c-6e58e8476c53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VISUALIZACIÓN DE RESULTADOS PARA QUERIES DE MUESTRA\n",
            "\n",
            "====================================================================================================\n",
            "QUERY ID: Q1\n",
            "QUERY TEXT: machine learning algorithms Papers about machine learning and deep learning algorithms\n",
            "====================================================================================================\n",
            "\n",
            "                                    RECUPERACIÓN INICIAL (Top-5)                                    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[1] Score: 0.4051 | Doc ID: 0708.2321\n",
            "    Fast learning rates for plug-in classifiers   It has been recently shown that, under the margin (or low noise) assumption,\n",
            "there exist classifiers att...\n",
            "\n",
            "[2] Score: 0.3910 | Doc ID: 0707.0303\n",
            "    Learning from dependent observations   In most papers establishing consistency for learning algorithms it is assumed\n",
            "that the observations used for tr...\n",
            "\n",
            "[3] Score: 0.3900 | Doc ID: 0709.1201\n",
            "    On the Proof Complexity of Deep Inference   We obtain two results about the proof complexity of deep inference: 1)\n",
            "deep-inference proof systems are as...\n",
            "\n",
            "[4] Score: 0.3865 | Doc ID: 0712.4126\n",
            "    TRUST-TECH based Methods for Optimization and Learning   Many problems that arise in machine learning domain deal with nonlinearity\n",
            "and quite often de...\n",
            "\n",
            "[5] Score: 0.3793 | Doc ID: 0712.4099\n",
            "    Digital Ecosystems: Optimisation by a Distributed Intelligence   Can intelligence optimise Digital Ecosystems? How could a distributed\n",
            "intelligence in...\n",
            "\n",
            "\n",
            "                                   DESPUÉS DE RE-RANKING (Top-5)                                    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[1] Score: 2.3719 | Doc ID: 0712.4126\n",
            "    TRUST-TECH based Methods for Optimization and Learning   Many problems that arise in machine learning domain deal with nonlinearity\n",
            "and quite often de...\n",
            "\n",
            "[2] Score: 1.4214 | Doc ID: 0707.0303\n",
            "    Learning from dependent observations   In most papers establishing consistency for learning algorithms it is assumed\n",
            "that the observations used for tr...\n",
            "\n",
            "[3] Score: 1.1026 | Doc ID: 0712.1027\n",
            "    Kernels and Ensembles: Perspectives on Statistical Learning   Since their emergence in the 1990's, the support vector machine and the\n",
            "AdaBoost algorit...\n",
            "\n",
            "[4] Score: 1.0504 | Doc ID: 0710.5896\n",
            "    Supervised Machine Learning with a Novel Pointwise Density Estimator   This article proposes a novel density estimation based algorithm for carrying\n",
            "o...\n",
            "\n",
            "[5] Score: 1.0442 | Doc ID: 0802.1412\n",
            "    Extreme Learning Machine for land cover classification   This paper explores the potential of extreme learning machine based\n",
            "supervised classification...\n",
            "\n",
            "====================================================================================================\n",
            "QUERY ID: Q2\n",
            "QUERY TEXT: quantum computing Research on quantum computing and quantum information\n",
            "====================================================================================================\n",
            "\n",
            "                                    RECUPERACIÓN INICIAL (Top-5)                                    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[1] Score: 0.7240 | Doc ID: 0705.3360\n",
            "    The Road to Quantum Artificial Intelligence   This paper overviews the basic principles and recent advances in the emerging\n",
            "field of Quantum Computati...\n",
            "\n",
            "[2] Score: 0.7078 | Doc ID: 0708.0261\n",
            "    An Introduction to Quantum Computing   Quantum Computing is a new and exciting field at the intersection of\n",
            "mathematics, computer science and physics....\n",
            "\n",
            "[3] Score: 0.6795 | Doc ID: 0712.1098\n",
            "    Quantum Computations: Fundamentals And Algorithms   Basic concepts of quantum theory of information, principles of quantum\n",
            "calculations and the possib...\n",
            "\n",
            "[4] Score: 0.6340 | Doc ID: 0705.3333\n",
            "    Simulation of Quantum Algorithms with a Symbolic Programming Language   This study examines the simulation of quantum algorithms on a classical\n",
            "comput...\n",
            "\n",
            "[5] Score: 0.6297 | Doc ID: 0705.4171\n",
            "    Grover search algorithm   A quantum algorithm is a set of instructions for a quantum computer, however,\n",
            "unlike algorithms in classical computer scienc...\n",
            "\n",
            "\n",
            "                                   DESPUÉS DE RE-RANKING (Top-5)                                    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[1] Score: 6.5947 | Doc ID: 0708.0261\n",
            "    An Introduction to Quantum Computing   Quantum Computing is a new and exciting field at the intersection of\n",
            "mathematics, computer science and physics....\n",
            "\n",
            "[2] Score: 5.7658 | Doc ID: 0705.4193\n",
            "    Lecture notes on Optical Quantum Computing   A quantum computer is a machine that can perform certain calculations much\n",
            "faster than a classical comput...\n",
            "\n",
            "[3] Score: 5.7596 | Doc ID: 0712.1098\n",
            "    Quantum Computations: Fundamentals And Algorithms   Basic concepts of quantum theory of information, principles of quantum\n",
            "calculations and the possib...\n",
            "\n",
            "[4] Score: 5.0383 | Doc ID: 0707.0324\n",
            "    Quantum Nash Equilibria and Quantum Computing   In this paper we review our earlier work on quantum computing and the Nash\n",
            "Equilibrium, in particular,...\n",
            "\n",
            "[5] Score: 4.9161 | Doc ID: 0705.1173\n",
            "    Effective Physical Processes and Active Information in Quantum Computing   The recent debate on hypercomputation has arisen new questions both on the\n",
            "...\n",
            "\n",
            "====================================================================================================\n",
            "QUERY ID: Q3\n",
            "QUERY TEXT: computer vision Papers about image processing and computer vision\n",
            "====================================================================================================\n",
            "\n",
            "                                    RECUPERACIÓN INICIAL (Top-5)                                    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[1] Score: 0.4187 | Doc ID: 0712.2854\n",
            "    Perception and recognition in a neural network theory in which neurons\n",
            "  exhibit hysteresis   A neural network theory of visual perception and recogni...\n",
            "\n",
            "[2] Score: 0.4107 | Doc ID: 0712.0121\n",
            "    Efficient Binary and Run Length Morphology and its Application to\n",
            "  Document Image Processing   This paper describes the implementation and evaluation...\n",
            "\n",
            "[3] Score: 0.3840 | Doc ID: 0801.2501\n",
            "    Mona Lisa, the stochastic view and fractality in color space   A painting consists of objects which are arranged in specific ways. The art\n",
            "of painting...\n",
            "\n",
            "[4] Score: 0.3816 | Doc ID: 0710.0410\n",
            "    The Theory of Unified Relativity for a Biovielectroluminescence\n",
            "  Phenomenon via Fly's Visual and Imaging System   The elucidation upon fly's neuronal...\n",
            "\n",
            "[5] Score: 0.3815 | Doc ID: 0708.2438\n",
            "    On Ullman's theorem in computer vision   Both in the plane and in space, we invert the nonlinear Ullman transformation\n",
            "for 3 points and 3 orthographic...\n",
            "\n",
            "\n",
            "                                   DESPUÉS DE RE-RANKING (Top-5)                                    \n",
            "----------------------------------------------------------------------------------------------------\n",
            "\n",
            "[1] Score: 2.6337 | Doc ID: 0708.0927\n",
            "    Modeling Visual Information Processing in Brain: A Computer Vision Point\n",
            "  of View and Approach   We live in the Information Age, and information has ...\n",
            "\n",
            "[2] Score: -1.3263 | Doc ID: 0705.0204\n",
            "    Using Images to create a Hierarchical Grid Spatial Index   This paper presents a hybrid approach to spatial indexing of two dimensional\n",
            "data. It sheds...\n",
            "\n",
            "[3] Score: -1.5442 | Doc ID: 0712.0137\n",
            "    View Based Methods can achieve Bayes-Optimal 3D Recognition   This paper proves that visual object recognition systems using only 2D\n",
            "Euclidean similar...\n",
            "\n",
            "[4] Score: -2.8754 | Doc ID: 0711.0784\n",
            "    Addendum to Research MMMCV; A Man/Microbio/Megabio/Computer Vision   In October 2007, a Research Proposal for the University of Sydney, Australia,\n",
            "the...\n",
            "\n",
            "[5] Score: -2.9446 | Doc ID: 0708.2438\n",
            "    On Ullman's theorem in computer vision   Both in the plane and in space, we invert the nonlinear Ullman transformation\n",
            "for 3 points and 3 orthographic...\n"
          ]
        }
      ],
      "source": [
        "# Mostrar resultados para las primeras 3 queries\n",
        "print(\"VISUALIZACIÓN DE RESULTADOS PARA QUERIES DE MUESTRA\")\n",
        "\n",
        "for i in range(min(3, len(query_ids))):\n",
        "    qid = query_ids[i]\n",
        "    qtext = query_texts[i]\n",
        "    initial = initial_results[qid]\n",
        "    reranked = reranked_results[qid]\n",
        "\n",
        "    display_query_results(qid, qtext, initial, reranked, top_n=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sec8_md",
      "metadata": {
        "id": "sec8_md"
      },
      "source": [
        "## 9. Evaluación del Sistema\n",
        "\n",
        "Evaluar la calidad del sistema usando:\n",
        "- **Precision@k:** Proporción de documentos relevantes en los top-k resultados\n",
        "- **Recall@k:** Proporción de documentos relevantes recuperados del total de relevantes\n",
        "\n",
        "Comparamos el rendimiento antes y después del re-ranking."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "evaluation_functions",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evaluation_functions",
        "outputId": "c6320cd5-bea3-490e-a86a-495dfda8b7bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Funciones de evaluación definidas\n"
          ]
        }
      ],
      "source": [
        "# Funciones de evaluación\n",
        "def precision_at_k(retrieved_docs, relevant_docs, k):\n",
        "    if k == 0 or len(retrieved_docs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    top_k = retrieved_docs[:k]\n",
        "    relevant_retrieved = sum(1 for doc_id in top_k if doc_id in relevant_docs)\n",
        "\n",
        "    return relevant_retrieved / k\n",
        "\n",
        "def recall_at_k(retrieved_docs, relevant_docs, k):\n",
        "    if len(relevant_docs) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    top_k = retrieved_docs[:k]\n",
        "    relevant_retrieved = sum(1 for doc_id in top_k if doc_id in relevant_docs)\n",
        "\n",
        "    return relevant_retrieved / len(relevant_docs)\n",
        "\n",
        "def evaluate_results(results_dict, qrels_dict, k_values=[5, 10, 20]):\n",
        "    metrics = {k: {'precision': [], 'recall': []} for k in k_values}\n",
        "\n",
        "    for query_id in results_dict:\n",
        "        if query_id not in qrels_dict:\n",
        "            continue\n",
        "\n",
        "        # Obtener documentos recuperados\n",
        "        retrieved = [doc_id for doc_id, _ in results_dict[query_id]]\n",
        "\n",
        "        # Obtener documentos relevantes (relevance > 0)\n",
        "        relevant = set([doc_id for doc_id, rel in qrels_dict[query_id].items() if rel > 0])\n",
        "\n",
        "        if len(relevant) == 0:\n",
        "            continue\n",
        "\n",
        "        # Calcular métricas para cada k\n",
        "        for k in k_values:\n",
        "            prec = precision_at_k(retrieved, relevant, k)\n",
        "            rec = recall_at_k(retrieved, relevant, k)\n",
        "            metrics[k]['precision'].append(prec)\n",
        "            metrics[k]['recall'].append(rec)\n",
        "\n",
        "    # Calcular promedios\n",
        "    avg_metrics = {}\n",
        "    for k in k_values:\n",
        "        avg_metrics[k] = {\n",
        "            'precision': np.mean(metrics[k]['precision']) if metrics[k]['precision'] else 0.0,\n",
        "            'recall': np.mean(metrics[k]['recall']) if metrics[k]['recall'] else 0.0\n",
        "        }\n",
        "\n",
        "    return avg_metrics\n",
        "\n",
        "print(\"Funciones de evaluación definidas\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "evaluate_initial",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evaluate_initial",
        "outputId": "dba8cba8-85d4-4afa-d816-56d0f83e47e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MÉTRICAS - RECUPERACIÓN INICIAL\n",
            "\n",
            "k = 5:\n",
            "  Precision@5: 0.7600\n",
            "  Recall@5:    0.0013\n",
            "\n",
            "k = 10:\n",
            "  Precision@10: 0.7500\n",
            "  Recall@10:    0.0027\n",
            "\n",
            "k = 20:\n",
            "  Precision@20: 0.7850\n",
            "  Recall@20:    0.0059\n"
          ]
        }
      ],
      "source": [
        "# Evaluar resultados de recuperación inicial\n",
        "k_values = [5, 10, 20]\n",
        "initial_metrics = evaluate_results(initial_results, qrels_dict, k_values)\n",
        "\n",
        "print(\"MÉTRICAS - RECUPERACIÓN INICIAL\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nk = {k}:\")\n",
        "    print(f\"  Precision@{k}: {initial_metrics[k]['precision']:.4f}\")\n",
        "    print(f\"  Recall@{k}:    {initial_metrics[k]['recall']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "evaluate_reranked",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "evaluate_reranked",
        "outputId": "fa106f39-5dc3-4d54-c515-0a514f46d3c4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MÉTRICAS - DESPUÉS DE RE-RANKING\n",
            "\n",
            "k = 5:\n",
            "  Precision@5: 0.8200\n",
            "  Recall@5:    0.0016\n",
            "\n",
            "k = 10:\n",
            "  Precision@10: 0.7800\n",
            "  Recall@10:    0.0030\n",
            "\n",
            "k = 20:\n",
            "  Precision@20: 0.3900\n",
            "  Recall@20:    0.0030\n"
          ]
        }
      ],
      "source": [
        "# Evaluar resultados después de re-ranking\n",
        "\n",
        "reranked_metrics = evaluate_results(reranked_results, qrels_dict, k_values)\n",
        "\n",
        "print(\"MÉTRICAS - DESPUÉS DE RE-RANKING\")\n",
        "\n",
        "for k in k_values:\n",
        "    print(f\"\\nk = {k}:\")\n",
        "    print(f\"  Precision@{k}: {reranked_metrics[k]['precision']:.4f}\")\n",
        "    print(f\"  Recall@{k}:    {reranked_metrics[k]['recall']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "compare_metrics",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "compare_metrics",
        "outputId": "10d5224f-7066-480d-8fb7-d22a155dfb57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "COMPARACIÓN: RECUPERACIÓN INICIAL vs RE-RANKING\n",
            "\n",
            "\n",
            "     Métrica Inicial Re-ranking Mejora (%)\n",
            " Precision@5  0.7600     0.8200     +7.89%\n",
            "    Recall@5  0.0013     0.0016    +29.72%\n",
            "Precision@10  0.7500     0.7800     +4.00%\n",
            "   Recall@10  0.0027     0.0030    +10.50%\n",
            "Precision@20  0.7850     0.3900    -50.32%\n",
            "   Recall@20  0.0059     0.0030    -48.48%\n"
          ]
        }
      ],
      "source": [
        "# Comparación lado a lado\n",
        "print(\"COMPARACIÓN: RECUPERACIÓN INICIAL vs RE-RANKING\")\n",
        "\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Métrica': [],\n",
        "    'Inicial': [],\n",
        "    'Re-ranking': [],\n",
        "    'Mejora (%)': []\n",
        "})\n",
        "\n",
        "for k in k_values:\n",
        "    for metric in ['precision', 'recall']:\n",
        "        initial_val = initial_metrics[k][metric]\n",
        "        reranked_val = reranked_metrics[k][metric]\n",
        "        improvement = ((reranked_val - initial_val) / initial_val * 100) if initial_val > 0 else 0\n",
        "\n",
        "        comparison_df = pd.concat([comparison_df, pd.DataFrame({\n",
        "            'Métrica': [f\"{metric.capitalize()}@{k}\"],\n",
        "            'Inicial': [f\"{initial_val:.4f}\"],\n",
        "            'Re-ranking': [f\"{reranked_val:.4f}\"],\n",
        "            'Mejora (%)': [f\"{improvement:+.2f}%\"]\n",
        "        })], ignore_index=True)\n",
        "\n",
        "print(\"\\n\")\n",
        "print(comparison_df.to_string(index=False))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "La comparación muestra que el re-ranking mejora el rendimiento en los primeros puestos del ranking, que son los más importantes desde el punto de vista del usuario. Se observa un aumento consistente en Precision@5 y Precision@10, indicando que el modelo de re-ranking logra colocar más documentos relevantes"
      ],
      "metadata": {
        "id": "BTkjbJakth-n"
      },
      "id": "BTkjbJakth-n"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### CALIDAD DE LA RECUPERACIÓN INICIAL:\n"
      ],
      "metadata": {
        "id": "4SjaxwUZrXhp"
      },
      "id": "4SjaxwUZrXhp"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La recuperación inicial basada en embeddings semánticos (Sentence-BERT) proporciona una primera selección de documentos candidatos usando similitud vectorial."
      ],
      "metadata": {
        "id": "JWnkKDVNrk3d"
      },
      "id": "JWnkKDVNrk3d"
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"- Precision@10 inicial: {initial_metrics[10]['precision']:.4f}\")\n",
        "print(f\"- Recall@10 inicial:    {initial_metrics[10]['recall']:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tpOI39JRrWiO",
        "outputId": "25a0d345-b32c-4227-ed4d-ec40517113f5"
      },
      "id": "tpOI39JRrWiO",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Precision@10 inicial: 0.7500\n",
            "- Recall@10 inicial:    0.0027\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### IMPACTO DEL RE-RANKING:"
      ],
      "metadata": {
        "id": "-ymzgNUerzNl"
      },
      "id": "-ymzgNUerzNl"
    },
    {
      "cell_type": "markdown",
      "source": [
        "El re-ranking con Cross-Encoder mejora significativamente la calidad de los resultados. Los cross-encoders procesan query y documento conjuntamente, capturando interacciones más complejas y matices semánticos.\n"
      ],
      "metadata": {
        "id": "_5aQCvNbsBSF"
      },
      "id": "_5aQCvNbsBSF"
    },
    {
      "cell_type": "code",
      "source": [
        "precision_improvement = ((reranked_metrics[10]['precision'] - initial_metrics[10]['precision']) /\n",
        "                         initial_metrics[10]['precision'] * 100) if initial_metrics[10]['precision'] > 0 else 0\n",
        "recall_improvement = ((reranked_metrics[10]['recall'] - initial_metrics[10]['recall']) /\n",
        "                      initial_metrics[10]['recall'] * 100) if initial_metrics[10]['recall'] > 0 else 0\n",
        "\n",
        "print(f\"- Mejora en Precision@10: {precision_improvement:+.2f}%\")\n",
        "print(f\"- Mejora en Recall@10:    {recall_improvement:+.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3MS7vzJsFvK",
        "outputId": "0c9d00f0-9c43-410c-cfa6-c07dfd0c8c31"
      },
      "id": "r3MS7vzJsFvK",
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Mejora en Precision@10: +4.00%\n",
            "- Mejora en Recall@10:    +10.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "conclusion",
      "metadata": {
        "id": "conclusion"
      },
      "source": [
        "## Conclusión\n",
        "- Pipeline basado en embeddings\n",
        "- Balance entre eficiencia y precisión\n",
        "- Mejora medible con re-ranking\n",
        "- Escalable a corpus grandes\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tyyG9zVis7sc"
      },
      "id": "tyyG9zVis7sc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
